
# ğŸ§  arXiv Computer Science Chatbot

This project builds a **research chatbot** for arXiv Computer Science papers.  
It allows you to search, retrieve, and summarize relevant papers using SBERT embeddings, FAISS vector search, and Streamlit UI.

---

## ğŸš€ Features

âœ… Download and preprocess the arXiv metadata dataset  
âœ… Filter for Computer Science papers  
âœ… Clean and save the data  
âœ… Create sentence embeddings using `sentence-transformers`  
âœ… Build a FAISS vector store for fast retrieval  
âœ… Interactive Streamlit chatbot with summarization  
âœ… Deploy the chatbot using `ngrok` to get a public URL

---

## ğŸ“‚ Dataset

- Source: [arXiv Metadata on Kaggle](https://www.kaggle.com/datasets/Cornell-University/arxiv)
- File: `arxiv-metadata-oai-snapshot.json`

---

## ğŸ› ï¸ Installation

Run these commands step by step in **Google Colab**.

### ğŸ”§ Install dependencies
```bash
!pip install pandas tqdm
!pip install sentence-transformers faiss-cpu
!pip install streamlit transformers sentence-transformers faiss-cpu
!pip install pyngrok
!pip install kaggle
```

### ğŸ“„ Save dependencies
```bash
!pip freeze > requirements.txt
```

You can download it:
```python
from google.colab import files
files.download("requirements.txt")
```

---

## ğŸ”‘ Kaggle API Setup

1ï¸âƒ£ Download your `kaggle.json` from [Kaggle API](https://www.kaggle.com/settings/account)  
2ï¸âƒ£ Upload it:
```python
from google.colab import files
files.upload()
```

3ï¸âƒ£ Move and set permissions:
```bash
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
```

---

## ğŸ“¥ Download Dataset

```bash
!kaggle datasets download -d Cornell-University/arxiv
!unzip -o arxiv.zip
```

---

## ğŸ§¹ Preprocess Data

- Parse JSON
- Filter Computer Science papers
- Clean text
- Save as CSV (`cs_arxiv_cleaned.csv`)

---

## ğŸ”— Build Vector Store

- Create embeddings with `all-MiniLM-L6-v2`
- Build FAISS index
- Save index and metadata

---

## ğŸ’¬ Run the Chatbot

### ğŸŒŸ Local setup with Streamlit & ngrok

1ï¸âƒ£ Kill existing Streamlit processes (optional)
```bash
!pkill -f streamlit || echo "No Streamlit process to kill"
```

2ï¸âƒ£ Configure ngrok
```python
from pyngrok import ngrok
ngrok.kill()

NGROK_TOKEN = input("Enter your ngrok authtoken: ").strip()
os.environ["NGROK_AUTHTOKEN"] = NGROK_TOKEN
!ngrok config add-authtoken $NGROK_AUTHTOKEN
```

3ï¸âƒ£ Launch Streamlit
```python
from pyngrok import ngrok
import threading, os

public_url = ngrok.connect(8501)
print(f"ğŸŒ Streamlit app running at: {public_url}")

def run():
    os.system("streamlit run main.py")

thread = threading.Thread(target=run)
thread.start()
```

---

## ğŸ“œ File Structure

```
.
â”œâ”€â”€ cs_arxiv_cleaned.csv       # Cleaned dataset
â”œâ”€â”€ cs_arxiv_index.faiss       # FAISS index
â”œâ”€â”€ cs_arxiv_ids.pkl           # Paper IDs
â”œâ”€â”€ cs_arxiv_texts.pkl         # Text content
â”œâ”€â”€ main.py                    # Streamlit app (see below)
â””â”€â”€ requirements.txt           # Python dependencies
```

---

## ğŸ‘¨â€ğŸ’» Streamlit App (`main.py`)

The `main.py` contains:
- Query input box
- Retrieves top-5 relevant papers
- Optional summarization of abstracts using `distilbart-cnn-12-6`

---

## ğŸ“¦ Notes

- SBERT model: `all-MiniLM-L6-v2`
- Summarizer: `sshleifer/distilbart-cnn-12-6`
- Recommended to test on GPU runtime in Colab.

---

## ğŸ“„ License

This project is open-sourced under MIT License.

---

## âœ¨ Credits

- [Sentence Transformers](https://www.sbert.net/)
- [FAISS](https://faiss.ai/)
- [Streamlit](https://streamlit.io/)
- [arXiv Dataset](https://www.kaggle.com/datasets/Cornell-University/arxiv)

---

Happy Researching! ğŸŒŸ
