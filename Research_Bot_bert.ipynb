{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas tqdm\n",
        "!pip install sentence-transformers faiss-cpu\n",
        "!pip install streamlit transformers sentence-transformers faiss-cpu\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "b9mlGNnc_2XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "ajwDGLcatEAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"requirements.txt\")"
      ],
      "metadata": {
        "id": "o6Orz2vZtFUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle pandas tqdm"
      ],
      "metadata": {
        "id": "AiDtmPHzIy8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-0-XTAI8awG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# Upload your Kaggle API key\n",
        "print(\"üìÅ Please upload your kaggle.json API key file:\")\n",
        "files.upload()  # Upload kaggle.json\n",
        "\n",
        "# Set up Kaggle API\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download arXiv dataset from Kaggle\n",
        "!kaggle datasets download -d Cornell-University/arxiv\n",
        "!unzip -o arxiv.zip\n",
        "\n",
        "# Define path to the JSON file\n",
        "json_path = \"arxiv-metadata-oai-snapshot.json\"\n",
        "\n",
        "# Parse JSON line-by-line for performance\n",
        "def load_json_lines(path, max_lines=None):\n",
        "    data = []\n",
        "    with open(path, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if max_lines and i >= max_lines:\n",
        "                break\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "# Load a limited number of lines for faster prototyping\n",
        "print(\"‚è≥ Loading JSON file (this may take 2‚Äì5 minutes)...\")\n",
        "data = load_json_lines(json_path)\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Filter for Computer Science papers\n",
        "df_cs = df[df['categories'].str.contains('cs.')].copy()\n",
        "df_cs = df_cs[['id', 'title', 'abstract', 'categories', 'update_date']]\n",
        "\n",
        "# Clean the abstract and title text\n",
        "def clean_text(text):\n",
        "    return text.replace('\\n', ' ').replace('\\r', ' ').strip() if isinstance(text, str) else \"\"\n",
        "\n",
        "df_cs['title'] = df_cs['title'].apply(clean_text)\n",
        "df_cs['abstract'] = df_cs['abstract'].apply(clean_text)\n",
        "\n",
        "# Drop papers with empty abstracts\n",
        "df_cs.dropna(subset=['abstract'], inplace=True)\n",
        "\n",
        "# Save cleaned dataset\n",
        "csv_path = \"cs_arxiv_cleaned.csv\"\n",
        "df_cs.to_csv(csv_path, index=False)\n",
        "print(f\" Done! Saved {len(df_cs)} Computer Science papers to `{csv_path}`.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"cs_arxiv_cleaned.csv\")\n",
        "texts = (df['title'] + \": \" + df['abstract']).tolist()\n",
        "ids = df['id'].tolist()\n",
        "\n",
        "# Embed with SBERT\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
        "\n",
        "# Create FAISS index\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "\n",
        "# Save index and data\n",
        "faiss.write_index(index, \"cs_arxiv_index.faiss\")\n",
        "with open(\"cs_arxiv_ids.pkl\", \"wb\") as f:\n",
        "    pickle.dump(ids, f)\n",
        "with open(\"cs_arxiv_texts.pkl\", \"wb\") as f:\n",
        "    pickle.dump(texts, f)\n",
        "\n",
        "print(\"Saved FAISS index and associated metadata\")"
      ],
      "metadata": {
        "id": "jfYoE7aE_3WT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pickle\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load vector store and metadata\n",
        "index = faiss.read_index(\"cs_arxiv_index.faiss\")\n",
        "texts = pickle.load(open(\"cs_arxiv_texts.pkl\", \"rb\"))\n",
        "ids = pickle.load(open(\"cs_arxiv_ids.pkl\", \"rb\"))\n",
        "\n",
        "# Load embedder and summarizer\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
        "\n",
        "st.set_page_config(page_title=\"üß† arXiv CS Chatbot\", layout=\"wide\")\n",
        "st.title(\"üß† arXiv Computer Science Chatbot\")\n",
        "\n",
        "query = st.text_input(\"Enter your research question or paper topic\")\n",
        "\n",
        "if query:\n",
        "    q_emb = embedder.encode([query])\n",
        "    D, I = index.search(np.array(q_emb), 5)\n",
        "\n",
        "    for idx in I[0]:\n",
        "        st.subheader(\"üîç Related Paper\")\n",
        "        st.write(texts[idx])\n",
        "\n",
        "        if st.button(f\"Summarize Paper {ids[idx]}\", key=str(idx)):\n",
        "            summary = summarizer(texts[idx][:1024])[0]['summary_text']\n",
        "            st.success(\"üìÑ Summary:\")\n",
        "            st.write(summary)"
      ],
      "metadata": {
        "id": "jSUq9szGACyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f streamlit || echo \"No Streamlit process to kill\"\n",
        "from pyngrok import ngrok\n",
        "ngrok.kill()\n",
        "\n",
        "NGROK_TOKEN = input(\"Enter your ngrok authtoken: \").strip()\n",
        "os.environ[\"NGROK_AUTHTOKEN\"] = NGROK_TOKEN\n",
        "\n",
        "!ngrok config add-authtoken $NGROK_AUTHTOKEN"
      ],
      "metadata": {
        "id": "gLaYIMEB9z6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import os\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"üåê Streamlit app running at: {public_url}\")\n",
        "\n",
        "def run():\n",
        "    os.system(\"streamlit run main.py\")\n",
        "\n",
        "thread = threading.Thread(target=run)\n",
        "thread.start()\n"
      ],
      "metadata": {
        "id": "tyx75OGI91rA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}